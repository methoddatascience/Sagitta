{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import spacy\n",
    "import itertools as it\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from gensim import corpora\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en') \n",
    "## python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = pd.read_csv(\"/Volumes/Transcend/MDS_Yelp/yelp_review.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>parsed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1155267</th>\n",
       "      <td>s9GBl-WBsCJrl49llUfTig</td>\n",
       "      <td>psaaZ3Ulc3EFGv37UpxxRg</td>\n",
       "      <td>SOoNoV7_YJcuWgVfetspfg</td>\n",
       "      <td>2</td>\n",
       "      <td>2015-08-04</td>\n",
       "      <td>I would be giving zero stars if my second experience was my only experience. On this second occa...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3007981</th>\n",
       "      <td>NMz3UJtl4ruL39QQGDv5ZQ</td>\n",
       "      <td>TudUls_yRpkZoDe9m9O_tg</td>\n",
       "      <td>7svPiqFz7ad82VcdDbBcJw</td>\n",
       "      <td>5</td>\n",
       "      <td>2017-07-04</td>\n",
       "      <td>I love this Starbucks, nice peeps and the service is outstanding.\\nThey know their business that...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1921077</th>\n",
       "      <td>6fNzrwuJyUnDtR8BcqNRfQ</td>\n",
       "      <td>5MChqlj0L3DzVi5-cS_xvg</td>\n",
       "      <td>CZKHXlDuy3IagC2W881fyA</td>\n",
       "      <td>5</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>So I've never been big on cocktails. I know they're good but i never really raves about how good...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788957</th>\n",
       "      <td>4YU22XKLI9z8oHOIur-c9w</td>\n",
       "      <td>iQbqQwF6ttvdMbpbqEGIaQ</td>\n",
       "      <td>onoZsIFPJXumpV_ndxcGTQ</td>\n",
       "      <td>2</td>\n",
       "      <td>2007-04-08</td>\n",
       "      <td>When I first came to town, I was stuck going here for my internet access.\\n\\nI had to pay for a ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274477</th>\n",
       "      <td>NbNhfEJoqLcqnz6dfwHeTQ</td>\n",
       "      <td>lSjdYb2wussdNmj5XvN45Q</td>\n",
       "      <td>01fuY2NNscttoTxOYbuZXw</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-06-10</td>\n",
       "      <td>I have only had the opportunity to eat here a couple times but it is on the top of the list for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      review_id                 user_id  \\\n",
       "1155267  s9GBl-WBsCJrl49llUfTig  psaaZ3Ulc3EFGv37UpxxRg   \n",
       "3007981  NMz3UJtl4ruL39QQGDv5ZQ  TudUls_yRpkZoDe9m9O_tg   \n",
       "1921077  6fNzrwuJyUnDtR8BcqNRfQ  5MChqlj0L3DzVi5-cS_xvg   \n",
       "788957   4YU22XKLI9z8oHOIur-c9w  iQbqQwF6ttvdMbpbqEGIaQ   \n",
       "274477   NbNhfEJoqLcqnz6dfwHeTQ  lSjdYb2wussdNmj5XvN45Q   \n",
       "\n",
       "                    business_id  stars        date  \\\n",
       "1155267  SOoNoV7_YJcuWgVfetspfg      2  2015-08-04   \n",
       "3007981  7svPiqFz7ad82VcdDbBcJw      5  2017-07-04   \n",
       "1921077  CZKHXlDuy3IagC2W881fyA      5  2017-12-11   \n",
       "788957   onoZsIFPJXumpV_ndxcGTQ      2  2007-04-08   \n",
       "274477   01fuY2NNscttoTxOYbuZXw      5  2015-06-10   \n",
       "\n",
       "                                                                                                        text  \\\n",
       "1155267  I would be giving zero stars if my second experience was my only experience. On this second occa...   \n",
       "3007981  I love this Starbucks, nice peeps and the service is outstanding.\\nThey know their business that...   \n",
       "1921077  So I've never been big on cocktails. I know they're good but i never really raves about how good...   \n",
       "788957   When I first came to town, I was stuck going here for my internet access.\\n\\nI had to pay for a ...   \n",
       "274477   I have only had the opportunity to eat here a couple times but it is on the top of the list for ...   \n",
       "\n",
       "         useful  funny  cool parsed_text  \n",
       "1155267       5      1     0        None  \n",
       "3007981       0      0     0        None  \n",
       "1921077       0      0     0        None  \n",
       "788957        2      1     0        None  \n",
       "274477        0      1     0        None  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nv_reviews = review.sample(5000)\n",
    "nv_reviews['parsed_text'] = None\n",
    "nv_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nv_reviews['text'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuation or whitespace\n",
    "    \"\"\"\n",
    "    \n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "def line_review(filename):\n",
    "    \"\"\"\n",
    "    generator function to read in reviews from the file\n",
    "    and un-escape the original line breaks in the text\n",
    "    \"\"\"\n",
    "    \n",
    "    with codecs.open(filename, encoding='utf_8') as f:\n",
    "        for review in f:\n",
    "            yield review.replace('\\\\n', '\\n')\n",
    "            \n",
    "def lemmatized_sentence_corpus(filename):\n",
    "    \"\"\"\n",
    "    generator function to use spaCy to parse reviews,\n",
    "    lemmatize the text, and yield sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    for parsed_review in nlp.pipe(line_review(filename),\n",
    "                                  batch_size=10000, n_threads=4):\n",
    "        \n",
    "        for sent in parsed_review.sents:\n",
    "            yield u' '.join([token.lemma_ for token in sent\n",
    "                             if not punct_space(token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = nv_reviews['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for i in range(len(documents)):\n",
    "    docs.append(nlp(documents[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I would be giving zero stars if my second experience was my only experience. On this second occasion I had barely walked in when told \"the guy that cuts your kind of hair isn\\'t here, come back tomorrow.\" \\n\\nMy haircut is so simple that any barber can do it. Any. Same length all the way around with a little extra left on the front top. No fades, no special shapes cut into the side of my head. Real simple. \\n\\nThe heavyset man there didn\\'t ask what style I wanted, just presumed black guy wants a black guy haircut. I was coming here because it was closer to my place than my regular place, Nate\\'s Barber Shop on 7th Ave. \\n\\nThank you for reminding me why it\\'s worth it to me to go to my regular place and stay the loyal customer I am. I\\'ll never go back to Steve\\'s Barber Shop. \\n\\nLest anyone think I\\'m implying racism, I\\'m not. You can come to your own conclusions. I\\'m invoking Hanlon\\'s Razor: never presume malice where stupidity is more likely.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I would be giving zero stars if my second experience was my only experience. On this second occasion I had barely walked in when told \"the guy that cuts your kind of hair isn't here, come back tomorrow.\" \n",
       "\n",
       "My haircut is so simple that any barber can do it. Any. Same length all the way around with a little extra left on the front top. No fades, no special shapes cut into the side of my head. Real simple. \n",
       "\n",
       "The heavyset man there didn't ask what style I wanted, just presumed black guy wants a black guy haircut. I was coming here because it was closer to my place than my regular place, Nate's Barber Shop on 7th Ave. \n",
       "\n",
       "Thank you for reminding me why it's worth it to me to go to my regular place and stay the loyal customer I am. I'll never go back to Steve's Barber Shop. \n",
       "\n",
       "Lest anyone think I'm implying racism, I'm not. You can come to your own conclusions. I'm invoking Hanlon's Razor: never presume malice where stupidity is more likely."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lemma = [token.lemma_ for token in docs[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_review = [token.lemma_ for token in docs[0] if not punct_space(token)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any remaining stopwords\n",
    "unigram_review = [term for term in unigram_review if term not in spacy.lang.en.STOP_WORDS]\n",
    "# nlp.Defaults.stop_words works as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-PRON-',\n",
       " 'zero',\n",
       " 'star',\n",
       " '-PRON-',\n",
       " 'second',\n",
       " 'experience',\n",
       " '-PRON-',\n",
       " 'experience',\n",
       " 'second',\n",
       " 'occasion',\n",
       " '-PRON-',\n",
       " 'barely',\n",
       " 'walk',\n",
       " 'tell',\n",
       " 'guy',\n",
       " 'cut',\n",
       " '-PRON-',\n",
       " 'kind',\n",
       " 'hair',\n",
       " 'come',\n",
       " 'tomorrow',\n",
       " '-PRON-',\n",
       " 'haircut',\n",
       " 'simple',\n",
       " 'barber',\n",
       " '-PRON-',\n",
       " 'length',\n",
       " 'way',\n",
       " 'little',\n",
       " 'extra',\n",
       " 'left',\n",
       " 'fade',\n",
       " 'special',\n",
       " 'shape',\n",
       " 'cut',\n",
       " '-PRON-',\n",
       " 'head',\n",
       " 'real',\n",
       " 'simple',\n",
       " 'heavyset',\n",
       " 'man',\n",
       " 'ask',\n",
       " 'style',\n",
       " '-PRON-',\n",
       " 'want',\n",
       " 'presume',\n",
       " 'black',\n",
       " 'guy',\n",
       " 'want',\n",
       " 'black',\n",
       " 'guy',\n",
       " 'haircut',\n",
       " '-PRON-',\n",
       " 'come',\n",
       " '-PRON-',\n",
       " 'close',\n",
       " '-PRON-',\n",
       " 'place',\n",
       " '-PRON-',\n",
       " 'regular',\n",
       " 'place',\n",
       " 'nate',\n",
       " \"'s\",\n",
       " 'barber',\n",
       " 'shop',\n",
       " '7th',\n",
       " 'ave',\n",
       " 'thank',\n",
       " '-PRON-',\n",
       " 'remind',\n",
       " '-PRON-',\n",
       " '-PRON-',\n",
       " 'worth',\n",
       " '-PRON-',\n",
       " '-PRON-',\n",
       " '-PRON-',\n",
       " 'regular',\n",
       " 'place',\n",
       " 'stay',\n",
       " 'loyal',\n",
       " 'customer',\n",
       " '-PRON-',\n",
       " '-PRON-',\n",
       " 'steve',\n",
       " \"'s\",\n",
       " 'barber',\n",
       " 'shop',\n",
       " 'lest',\n",
       " 'think',\n",
       " '-PRON-',\n",
       " 'imply',\n",
       " 'racism',\n",
       " '-PRON-',\n",
       " '-PRON-',\n",
       " 'come',\n",
       " '-PRON-',\n",
       " 'conclusion',\n",
       " '-PRON-',\n",
       " 'invoke',\n",
       " 'hanlon',\n",
       " \"'s\",\n",
       " 'razor',\n",
       " 'presume',\n",
       " 'malice',\n",
       " 'stupidity',\n",
       " 'likely']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "sentences = Text8Corpus(datapath('testcorpus.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Text8Corpus at 0x1a198d6390>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/anaconda3/lib/python3.6/site-packages/gensim/test/test_data/testcorpus.txt'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath('testcorpus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='/anaconda3/lib/python3.6/site-packages/gensim/test/test_data/testcorpus.txt' mode='r' encoding='UTF-8'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open('/anaconda3/lib/python3.6/site-packages/gensim/test/test_data/testcorpus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.phrases.Phrases at 0x1a198d6ac8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases = Phrases(sentences, min_count=1, threshold=1)\n",
    "phrases\n",
    "#https://stackoverflow.com/questions/35716121/how-to-extract-phrases-from-corpus-using-gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['trees_graph', 'minors']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases[[u'trees', u'graph', u'minors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>parsed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s9GBl-WBsCJrl49llUfTig</td>\n",
       "      <td>psaaZ3Ulc3EFGv37UpxxRg</td>\n",
       "      <td>SOoNoV7_YJcuWgVfetspfg</td>\n",
       "      <td>2</td>\n",
       "      <td>2015-08-04</td>\n",
       "      <td>I would be giving zero stars if my second experience was my only experience. On this second occa...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NMz3UJtl4ruL39QQGDv5ZQ</td>\n",
       "      <td>TudUls_yRpkZoDe9m9O_tg</td>\n",
       "      <td>7svPiqFz7ad82VcdDbBcJw</td>\n",
       "      <td>5</td>\n",
       "      <td>2017-07-04</td>\n",
       "      <td>I love this Starbucks, nice peeps and the service is outstanding.\\nThey know their business that...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6fNzrwuJyUnDtR8BcqNRfQ</td>\n",
       "      <td>5MChqlj0L3DzVi5-cS_xvg</td>\n",
       "      <td>CZKHXlDuy3IagC2W881fyA</td>\n",
       "      <td>5</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>So I've never been big on cocktails. I know they're good but i never really raves about how good...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4YU22XKLI9z8oHOIur-c9w</td>\n",
       "      <td>iQbqQwF6ttvdMbpbqEGIaQ</td>\n",
       "      <td>onoZsIFPJXumpV_ndxcGTQ</td>\n",
       "      <td>2</td>\n",
       "      <td>2007-04-08</td>\n",
       "      <td>When I first came to town, I was stuck going here for my internet access.\\n\\nI had to pay for a ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NbNhfEJoqLcqnz6dfwHeTQ</td>\n",
       "      <td>lSjdYb2wussdNmj5XvN45Q</td>\n",
       "      <td>01fuY2NNscttoTxOYbuZXw</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-06-10</td>\n",
       "      <td>I have only had the opportunity to eat here a couple times but it is on the top of the list for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  s9GBl-WBsCJrl49llUfTig  psaaZ3Ulc3EFGv37UpxxRg  SOoNoV7_YJcuWgVfetspfg   \n",
       "1  NMz3UJtl4ruL39QQGDv5ZQ  TudUls_yRpkZoDe9m9O_tg  7svPiqFz7ad82VcdDbBcJw   \n",
       "2  6fNzrwuJyUnDtR8BcqNRfQ  5MChqlj0L3DzVi5-cS_xvg  CZKHXlDuy3IagC2W881fyA   \n",
       "3  4YU22XKLI9z8oHOIur-c9w  iQbqQwF6ttvdMbpbqEGIaQ  onoZsIFPJXumpV_ndxcGTQ   \n",
       "4  NbNhfEJoqLcqnz6dfwHeTQ  lSjdYb2wussdNmj5XvN45Q  01fuY2NNscttoTxOYbuZXw   \n",
       "\n",
       "   stars        date  \\\n",
       "0      2  2015-08-04   \n",
       "1      5  2017-07-04   \n",
       "2      5  2017-12-11   \n",
       "3      2  2007-04-08   \n",
       "4      5  2015-06-10   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0  I would be giving zero stars if my second experience was my only experience. On this second occa...   \n",
       "1  I love this Starbucks, nice peeps and the service is outstanding.\\nThey know their business that...   \n",
       "2  So I've never been big on cocktails. I know they're good but i never really raves about how good...   \n",
       "3  When I first came to town, I was stuck going here for my internet access.\\n\\nI had to pay for a ...   \n",
       "4  I have only had the opportunity to eat here a couple times but it is on the top of the list for ...   \n",
       "\n",
       "   useful  funny  cool parsed_text  \n",
       "0       5      1     0        None  \n",
       "1       0      0     0        None  \n",
       "2       0      0     0        None  \n",
       "3       2      1     0        None  \n",
       "4       0      1     0        None  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nv_reviews = nv_reviews.reset_index(drop=True)\n",
    "nv_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text from 5,000 restaurant reviews written to the new txt file.\n"
     ]
    }
   ],
   "source": [
    "review_txt_filepath = os.path.join('review_text_all.txt')\n",
    "\n",
    "review_count = 0\n",
    "\n",
    "# create & open a new file in write mode\n",
    "with codecs.open(review_txt_filepath, 'w', encoding='utf_8') as review_txt_file:\n",
    "    # if this review is not about a restaurant, skip to the next one\n",
    "    # write the restaurant review as a line in the new file\n",
    "    # escape newline characters in the original review text\n",
    "    for i in range(len(nv_reviews['text'])):\n",
    "        review_txt_file.write(nv_reviews['text'][i].replace('\\n', '\\\\n') + '\\n')\n",
    "        review_count += 1\n",
    "\n",
    "print(u'''Text from {:,} restaurant reviews written to the new txt file.'''.format(review_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       I would be giving zero stars if my second experience was my only experience. On this second occa...\n",
       "1       I love this Starbucks, nice peeps and the service is outstanding.\\nThey know their business that...\n",
       "2       So I've never been big on cocktails. I know they're good but i never really raves about how good...\n",
       "3       When I first came to town, I was stuck going here for my internet access.\\n\\nI had to pay for a ...\n",
       "4       I have only had the opportunity to eat here a couple times but it is on the top of the list for ...\n",
       "5       Best Thai food! Always fresh and consistent. Food is always hot and made to order. Lunch special...\n",
       "6       We absolutely love Delux - especially the sweet potato fries and the Delux Burger.  It's not unc...\n",
       "7       Dr Budd and his team are the best. You will never regret getting orthodontic care by him. He is ...\n",
       "8       Excellent calamari and Cal Italia pizza!! Sat in the sports bar area.  Conveniently located by F...\n",
       "9       I was so happy I got the chance to visit Lufa Farms sewing that I've been a member for over a ye...\n",
       "10      My husband and I were so taken with the beauty of the live edge wood. We had a coffee table top ...\n",
       "11      Food City is terrific!  When you go (and you should) think \"fresh\" - skipping the jars, cans, an...\n",
       "12      I pay 5$, took a sip and this is what is leftover. Just....just.... ridiculous!!! And look at th...\n",
       "13      I liked the chicken quesadillas at Margaritaville. Very cheesy and meaty. My friend had the beef...\n",
       "14      As with the other Chipotles I've been to, it was GREAT! Came here for lunch in between baseball ...\n",
       "15      Love European Wax!! I've been to a 3 locations & I like them all. I have found a technician in e...\n",
       "16      Excited to check this place out. Love bahn mi and French macaroons, and in one joint was golden....\n",
       "17                   Excellent food, beautiful interior and good music! I LOVE their kabobs! Another A+++++\n",
       "18      Take it from a real live Indian who used to live in New Delhi: this is the best, most authentic,...\n",
       "19      Over a month ago, I had a bad experience when I got stuck in the tunnel due to the manager negle...\n",
       "20      We hit this place about 3 AM after some clubbing.  They have lots of tasty drink selections as w...\n",
       "21      Things started off great until I noticed more and more that my daughter was getting hurt. It wen...\n",
       "22      The food is always consistent and very good. The only thing that is a down fall is they don't de...\n",
       "23      Skip is an old dog, they were so sweet and gentle with him. I appreciated the extra time they to...\n",
       "24      Hey People...\\nThis Place Ain't Nothing Like Coyote Ugly.\\nCause Coyote Ugly NO MATTER HOW HARD ...\n",
       "25      Every time I take my car in, they tell me something is wrong with my car that is not wrong. Then...\n",
       "26      Was very excited to try this, who doesn't like cinnamon rolls.  I only tried the mini cini and m...\n",
       "27      I am struggling with exactly what to say about this place because I see so many rave reviews on ...\n",
       "28      HUGE! Friendly service. Peaceful atmosphere.\\n\\n I wish I could remember the name of the gentlem...\n",
       "29      Excellent experiences at the Avon location, getting my hair cut by Cami. She's fantastic and ver...\n",
       "                                                       ...                                                 \n",
       "4970    First off, let me say that I'm glad this place exists in bloomfield as the only other passable a...\n",
       "4971    This is a really cool place to drink with friends. We love the atmosphere of this bar. Very cool...\n",
       "4972    I love this place. I have been coming here for years. Best place to get your nails done and pedi...\n",
       "4973    My breakfast joint?\\n\\nYou mean the one that serves up a decent breakfast special (under 7 buck)...\n",
       "4974    I waited a few minutes at the counter to be helped at about 2:30 pm. I knocked on the counter. I...\n",
       "4975    This place has authentic kaachi gohst ki biryani, and it's absolutely divine! I've searched high...\n",
       "4976    The Yard is like a big recess for adults at night so it seems.So weird to see three restaurants ...\n",
       "4977    We stayed at this resort. Only downside, if you consider it a downside, it's about 10miles from ...\n",
       "4978    I wanted a second opinion on an inspection I received on my car and Joe did an awesome job and e...\n",
       "4979    Worst drive through experience, will avoid this place at all cost. Rude and unprofessional worke...\n",
       "4980    I'm not a fan of running to write a review of salons until I trust them implicitly. I've been bu...\n",
       "4981    I wish we had a Shake Shack back in Hawaii, but until then, I got my fill in Vegas.\\n\\nLocated r...\n",
       "4982    I came here because they have an on site window chip repair guy. My husband had recently come he...\n",
       "4983     A rip off ,staff is not friendly ,food and drinks overpriced,golf overrated.  And rooms are small.\n",
       "4984    Ok, so this place is a big box dental corporation. This is not your average individually owned\\/...\n",
       "4985    I found this place to be ok. The biggest problem is there is always a little queue on weekends. ...\n",
       "4986    I took my 2 dogs to this clinic on a recommendation and am glad I did. The support staff is know...\n",
       "4987    Excellent wings with more flavors than just 'mild' or 'hot'. I ordered garlic parm and lemon gar...\n",
       "4988    Overrated. Both food and service were wanting- there are several superior choices for Italian fa...\n",
       "4989           Always support your local USO even if you're not active duty or retired!  God Bless America!\n",
       "4990    Visiting Champaign on business and stumbled onto this great Irish pub.   First of all the decor ...\n",
       "4991    Food is good and delivery is quick. Eat from here often and have never had a bad experience. Try...\n",
       "4992    a surprising Thai place in the small town of Brunswick OH.  I was impressed with the d√©cor here ...\n",
       "4993    Usually when I order fish and chips at a place I end up with something that is more batter (see:...\n",
       "4994    This store has generally had good service but recently I wanted to purchase some window screens....\n",
       "4995    I recently took a cross country road trip and plotted out a few brewpubs to visit on beermapping...\n",
       "4996    This place is a joke. They make an appointment for you and then you have to wait 2 hours to be s...\n",
       "4997    The customer service and professionalism is terrible. I ordered items for a wedding. They gave m...\n",
       "4998    I got takeaway here the other night because I just moved into my flat and had no cooking supplie...\n",
       "4999    Sure, you could hate on LGO for:\\n-the difficulty of finding a parking spot (although people are...\n",
       "Name: text, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nv_reviews['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/rajeshpothamsetty/Sagitta/Reviewly'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepaths# filepat \n",
    "unigram_sentences_filepath = os.path.join('unigram.txt')\n",
    "bigram_sentences_filepath = os.path.join('bigrams.txt')\n",
    "bigram_model_filepath = os.path.join('bigram_model_all')\n",
    "trigram_sentences_filepath = os.path.join('trigrams.txt')\n",
    "trigram_model_filepath = os.path.join('trigram_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unigrams\n",
    "with codecs.open(unigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "    for sentence in lemmatized_sentence_corpus(review_txt_filepath):\n",
    "        f.write(sentence + '\\n')\n",
    "\n",
    "unigram_sentences = LineSentence(unigram_sentences_filepath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "data1 = range(10)\n",
    "\n",
    "# This creates a NEW list\n",
    "data1[2:5]\n",
    "\n",
    "# This creates an iterator that iterates over the EXISTING list\n",
    "for x in it.islice(data1, 2, 5):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.43 s, sys: 54.6 ms, total: 1.49 s\n",
      "Wall time: 1.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "use_model = 'no'\n",
    "# this is a bit time consuming - make the if statement True# this is \n",
    "# if you want to execute modeling yourself.\n",
    "if use_model == 'no':\n",
    "    bigram_model = Phrases(unigram_sentences)\n",
    "    bigram_model.save(bigram_model_filepath)\n",
    "else:    \n",
    "    # load the finished model from disk\n",
    "    bigram_model = Phrases.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.34 s, sys: 59.1 ms, total: 3.4 s\n",
      "Wall time: 3.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if use_model == 'no':\n",
    "    with codecs.open(bigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        for unigram_sentence in unigram_sentences:\n",
    "            bigram_sentence = u' '.join(bigram_model[unigram_sentence]) \n",
    "            f.write(bigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_sentences  = LineSentence(bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.44 s, sys: 59.2 ms, total: 1.5 s\n",
      "Wall time: 1.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "use_model = 'no'\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute modeling yourself.\n",
    "if use_model == 'no':\n",
    "    trigram_model = Phrases(bigram_sentences)\n",
    "    trigram_model.save(trigram_model_filepath)\n",
    "else:    \n",
    "    # load the finished model from disk\n",
    "    trigram_model = Phrases.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.2 s, sys: 41.5 ms, total: 3.24 s\n",
      "Wall time: 3.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if use_model == 'no':\n",
    "    with codecs.open(trigram_sentences_filepath, 'w', encoding='utf_8') as f:  \n",
    "        for bigram_sentence in bigram_sentences:        \n",
    "            trigram_sentence = u' '.join(trigram_model[bigram_sentence])       \n",
    "            f.write(trigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_sentences = LineSentence(trigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_reviews_filepath  = os.path.join('trigram_transformed_reviews_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 58s, sys: 48 s, total: 3min 46s\n",
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "use_model_trigram = 'no'\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if use_model_trigram == 'no':\n",
    "    with codecs.open(trigram_reviews_filepath, 'w', encoding='utf_8') as f:\n",
    "        for parsed_review in nlp.pipe(line_review(review_txt_filepath), batch_size=10000, n_threads=4):\n",
    "            \n",
    "            # lemmatize the text, removing punctuation and whitespace\n",
    "            unigram_review = [token.lemma_ for token in parsed_review if not punct_space(token)]\n",
    "            \n",
    "            # apply the first-order and second-order phrase models\n",
    "            bigram_review = bigram_model[unigram_review]\n",
    "            trigram_review = trigram_model[bigram_review]\n",
    "            \n",
    "            # remove any remaining stopwords\n",
    "            trigram_review = [term for term in trigram_review if term not in spacy.lang.en.STOP_WORDS]\n",
    "            \n",
    "            # write the transformed review as a line in the new file\n",
    "            trigram_review = u' '.join(trigram_review)\n",
    "            f.write(trigram_review + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "\n",
      "My husband and I were so taken with the beauty of the live edge wood. We had a coffee table top made and they even attached our mesquite legs to the top. It is so beautiful. Bjorn made the process so simple. I have been down there many times now and they always show the same interest in what you want to do.  They also do commercial work and some of their projects are just amazing. I plan to go to some of their DIY workshops. Terry Schuman\n",
      "\n",
      "<class 'str'>\n",
      "----\n",
      "\n",
      "Transformed:\n",
      "\n",
      "-PRON- husband -PRON- beauty live edge wood -PRON- coffee table -PRON- attach -PRON- mesquite leg -PRON- beautiful bjorn process simple -PRON- many_time -PRON- interest -PRON- want -PRON- commercial work -PRON- project amazing -PRON- plan -PRON- diy workshop terry schuman\n",
      "\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(u'Original:' + u'\\n')\n",
    "\n",
    "for review in it.islice(line_review(review_txt_filepath), 10, 11):\n",
    "    print(review)\n",
    "    print(type(review))\n",
    "\n",
    "print(u'----' + u'\\n')\n",
    "print(u'Transformed:' + u'\\n')\n",
    "\n",
    "with codecs.open(trigram_reviews_filepath, encoding='utf_8') as f:\n",
    "    for review in it.islice(f, 10, 11):\n",
    "        temp = review\n",
    "        print(review)\n",
    "        print(type(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-PRON- husband -PRON- beauty live edge wood -PRON- coffee table -PRON- attach -PRON- mesquite leg -PRON- beautiful bjorn process simple -PRON- many_time -PRON- interest -PRON- want -PRON- commercial work -PRON- project amazing -PRON- plan -PRON- diy workshop terry schuman\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pron',\n",
       " 'husband',\n",
       " 'pron',\n",
       " 'beauti',\n",
       " 'live',\n",
       " 'edg',\n",
       " 'wood',\n",
       " 'pron',\n",
       " 'coffe',\n",
       " 'tabl',\n",
       " 'pron',\n",
       " 'attach',\n",
       " 'pron',\n",
       " 'mesquit',\n",
       " 'leg',\n",
       " 'pron',\n",
       " 'beauti',\n",
       " 'bjorn',\n",
       " 'process',\n",
       " 'simpl',\n",
       " 'pron',\n",
       " 'manytim',\n",
       " 'pron',\n",
       " 'interest',\n",
       " 'pron',\n",
       " 'want',\n",
       " 'pron',\n",
       " 'commerci',\n",
       " 'work',\n",
       " 'pron',\n",
       " 'project',\n",
       " 'amaz',\n",
       " 'pron',\n",
       " 'plan',\n",
       " 'pron',\n",
       " 'diy',\n",
       " 'workshop',\n",
       " 'terri',\n",
       " 'schuman',\n",
       " '']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from linkedin course\n",
    "# not a great combination with tri-gram\n",
    "# should try independently\n",
    "x_temp = temp.replace('_', \" \")\n",
    "clean_text(temp) \n",
    "# remove 'pron'\n",
    "# add spell Checker\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pron',\n",
       " 'husband',\n",
       " 'pron',\n",
       " 'beauti',\n",
       " 'live',\n",
       " 'edg',\n",
       " 'wood',\n",
       " 'pron',\n",
       " 'coffe',\n",
       " 'tabl',\n",
       " 'pron',\n",
       " 'attach',\n",
       " 'pron',\n",
       " 'mesquit',\n",
       " 'leg',\n",
       " 'pron',\n",
       " 'beauti',\n",
       " 'bjorn',\n",
       " 'process',\n",
       " 'simpl',\n",
       " 'pron',\n",
       " 'mani',\n",
       " 'time',\n",
       " 'pron',\n",
       " 'interest',\n",
       " 'pron',\n",
       " 'want',\n",
       " 'pron',\n",
       " 'commerci',\n",
       " 'work',\n",
       " 'pron',\n",
       " 'project',\n",
       " 'amaz',\n",
       " 'pron',\n",
       " 'plan',\n",
       " 'pron',\n",
       " 'diy',\n",
       " 'workshop',\n",
       " 'terri',\n",
       " 'schuman',\n",
       " '']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(x_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "acac = open('trigram_transformed_reviews_all.txt', \"r\")\n",
    "lines = list(acac)\n",
    "lines_ = pd.Series(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       -PRON- zero_star -PRON- second experience -PRON- experience second occasion -PRON- barely walk_i...\n",
       "1       -PRON- love starbucks nice peep service outstanding -PRON- know -PRON- business sure -PRON- -PRO...\n",
       "2       -PRON- big cocktail -PRON- know -PRON- good rave_about good -PRON- come brickyard downtown -PRON...\n",
       "3       -PRON- come town -PRON- stick -PRON- internet access -PRON- pay monthly subscription use wired c...\n",
       "4       -PRON- opportunity eat_here couple time -PRON- list place go_back last_time -PRON- -PRON- pimp'n...\n",
       "5       good thai food fresh consistent food hot order lunch_special $ 6.95 come choice rice soup salad ...\n",
       "6       -PRON- absolutely_love delux especially sweet_potato_fry delux burger -PRON- uncommon -PRON- com...\n",
       "7       dr budd -PRON- team good -PRON- will_never regret orthodontic care -PRON- -PRON- patient kind -P...\n",
       "8       excellent calamari cal italia pizza sat sport_bar area conveniently_locate fremont street experi...\n",
       "9       -PRON- happy -PRON- chance visit lufa farms sew -PRON- member year disappoint service quality -P...\n",
       "10      -PRON- husband -PRON- beauty live edge wood -PRON- coffee table -PRON- attach -PRON- mesquite le...\n",
       "11      food city terrific -PRON- -PRON- think fresh skip jar prepackag stuff food literally of_course d...\n",
       "12                                                    -PRON- pay 5 $ sip leftover ridiculous look_at size\\n\n",
       "13      -PRON- like chicken quesadilla margaritaville cheesy meaty -PRON- friend beef -PRON- awful guaca...\n",
       "14      chipotles -PRON- -PRON- great come_here lunch baseball game padre field sport complex -PRON- sta...\n",
       "15      love european wax -PRON- 3 location -PRON- like -PRON- -PRON- find technician location -PRON- li...\n",
       "16      excited check this_place love bahn mi french macaroon joint golden a_bit disappointed macaroon -...\n",
       "17                           excellent food beautiful interior good music -PRON- love -PRON- kabob a+++++\\n\n",
       "18      -PRON- real live indian use live new delhi good authentic like mom -PRON- food in_phoenix grocer...\n",
       "19      month_ago -PRON- bad_experience -PRON- stick tunnel due_to manager neglect -PRON- duty ensure ex...\n",
       "20      -PRON- hit this_place 3 clubbing -PRON- lot_of tasty drink selection as_well_as clock breakfast ...\n",
       "21      thing start_off great -PRON- notice -PRON- daughter hurt -PRON- from_scratch bite bruise straw b...\n",
       "22      food consistent good only_thing fall -PRON- do_not deliver 5 other_than_that food great staff pl...\n",
       "23                  skip an_old dog -PRON- sweet gentle -PRON- -PRON- appreciate extra time -PRON- -PRON-\\n\n",
       "24      hey people this_place like coyote ugly cause coyote ugly no_matter hard they\"ve try will_never h...\n",
       "25      every_time -PRON- -PRON- car -PRON- tell -PRON- wrong -PRON- car wrong -PRON- proceed -PRON- rid...\n",
       "26      excited try do_not like cinnamon_roll -PRON- try mini cini maybe -PRON- different -PRON- cream_c...\n",
       "27      -PRON- struggle exactly_what this_place -PRON- so_many rave_review fabulous -PRON- pizza -PRON- ...\n",
       "28      huge friendly service peaceful atmosphere -PRON- wish -PRON- remember gentleman take_care_of -PR...\n",
       "29      excellent experience avon location -PRON- hair_cut cami -PRON- fantastic skilled mediocre cut -P...\n",
       "                                                       ...                                                 \n",
       "4970    first_off let -PRON- -PRON- glad this_place exist bloomfield passable alternative umi -PRON- upd...\n",
       "4971    really_cool place drink friend -PRON- love atmosphere bar cool jukebox bartender awesome friendl...\n",
       "4972                         -PRON- love_this_place -PRON- come_here year good place -PRON- nail pedicure\\n\n",
       "4973    -PRON- breakfast joint -PRON- mean serve decent breakfast special 7 buck 6 10 mon fri place spec...\n",
       "4974    -PRON- wait few_minute counter help 2:30 pm -PRON- knock counter -PRON- shout hello -PRON- shout...\n",
       "4975    this_place authentic kaachi gohst ki biryani -PRON- absolutely divine -PRON- search high low pla...\n",
       "4976    yard like big recess adult night -PRON- weird restaurant place -PRON- work -PRON- work -PRON- pa...\n",
       "4977    -PRON- stay_at resort only_downside -PRON- consider -PRON- downside -PRON- 10miles strip -PRON- ...\n",
       "4978    -PRON- want second opinion inspection -PRON- receive -PRON- car joe an_awesome job explain_every...\n",
       "4979       bad drive_through experience avoid this_place cost rude unprofessional worker food eh fry cold\\n\n",
       "4980    -PRON- fan_of run write review salon -PRON- trust -PRON- implicitly -PRON- burn -PRON- choose th...\n",
       "4981    -PRON- wish -PRON- shake shack hawaii -PRON- -PRON- fill in_vegas located right outside new_york...\n",
       "4982    -PRON- come_here -PRON- on_site window chip repair guy -PRON- husband recently come_here receive...\n",
       "4983                                rip_off staff friendly food drink overpriced golf overrate room small\\n\n",
       "4984    ok this_place big box dental corporation -PRON- average individually owned\\/family dental practi...\n",
       "4985    -PRON- find this_place ok big problem little queue weekend -PRON- egg good toronto many_other go...\n",
       "4986    -PRON- -PRON- 2 dog clinic recommendation glad -PRON- support staff knowledgeable caring -PRON- ...\n",
       "4987    excellent wing flavor mild hot -PRON- order garlic parm lemon garlic delicious fry surprisingly ...\n",
       "4988                  overrate food service wanting- superior choice italian fare pizza ft mill\\/tega cay\\n\n",
       "4989                                 support -PRON- local uso -PRON- active duty retire god bless america\\n\n",
       "4990    visit champaign business stumble great irish pub decor traditional irish second more_importantly...\n",
       "4991               food good delivery quick eat bad_experience try singapore rice noodles -PRON- eat_here\\n\n",
       "4992    surprising thai place small town brunswick oh -PRON- impressed_with d√©cor fact_that -PRON- famil...\n",
       "4993    usually -PRON- order fish chip place -PRON- end_up batter greasy dark brown crusty fish -PRON- s...\n",
       "4994    store generally good service recently -PRON- want purchase window screen -PRON- promptly receive...\n",
       "4995    -PRON- recently cross country road trip plot a_few brewpub visit beermapping.com -PRON- must_say...\n",
       "4996    this_place joke -PRON- make_an_appointment -PRON- -PRON- wait 2_hour -PRON- appointment -PRON- d...\n",
       "4997    customer_service professionalism terrible -PRON- order item wedding -PRON- -PRON- time date pick...\n",
       "4998    -PRON- takeaway night -PRON- -PRON- flat cook supply ok -PRON- do_not think -PRON- need an_excus...\n",
       "4999    sure -PRON- hate lgo -the difficulty find parking spot people come wait 2 minute vacate spot par...\n",
       "Length: 5000, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "X_tfidf = tfidf_vect.fit_transform(lines_)\n",
    "print(X_tfidf.shape)\n",
    "print(tfidf_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_sample = lines_[0:20]\n",
    "\n",
    "tfidf_vect_sample = TfidfVectorizer(analyzer=clean_text)\n",
    "X_tfidf_sample = tfidf_vect_sample.fit_transform(data_sample)\n",
    "print(X_tfidf_sample.shape)\n",
    "print(tfidf_vect_sample.get_feature_names())\n",
    "\n",
    "X_tfidf_df = pd.DataFrame(X_tfidf_sample.toarray())\n",
    "X_tfidf_df.columns = tfidf_vect_sample.get_feature_names()\n",
    "X_tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "dict_series = [0]*len(lines_)\n",
    "i = 0\n",
    "for rev in lines_:\n",
    "    fq= defaultdict(int)\n",
    "    for w in rev.split():\n",
    "        fq[w] += 1\n",
    "    dict_series[i] = fq\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'-PRON-': 18,\n",
       "             '-the': 3,\n",
       "             '2': 1,\n",
       "             '5_min': 1,\n",
       "             '5_star': 1,\n",
       "             'amazing': 1,\n",
       "             'and\\\\/or': 1,\n",
       "             'anymore': 1,\n",
       "             'anything_else': 1,\n",
       "             'astray': 1,\n",
       "             'bar': 1,\n",
       "             'base': 1,\n",
       "             'big': 1,\n",
       "             'blow': 1,\n",
       "             'breakfast_burrito': 1,\n",
       "             'cake': 1,\n",
       "             'choice': 1,\n",
       "             'coe': 1,\n",
       "             'coffee': 1,\n",
       "             'come': 1,\n",
       "             'commuter': 1,\n",
       "             'complaint': 1,\n",
       "             'confection': 1,\n",
       "             'cookie': 1,\n",
       "             'crowd': 1,\n",
       "             'cupcake': 2,\n",
       "             'dangerous': 1,\n",
       "             'dare': 1,\n",
       "             'daunting': 1,\n",
       "             'delicious': 1,\n",
       "             'difficulty': 1,\n",
       "             'dish': 1,\n",
       "             'do_not': 4,\n",
       "             'earth': 1,\n",
       "             'equally': 1,\n",
       "             'even_though': 1,\n",
       "             'experience': 1,\n",
       "             'feel': 1,\n",
       "             'finally': 1,\n",
       "             'find': 1,\n",
       "             'food': 2,\n",
       "             'friendly': 1,\n",
       "             'gelato': 1,\n",
       "             'gluten_free': 1,\n",
       "             'good': 3,\n",
       "             'gooey': 2,\n",
       "             'great': 1,\n",
       "             'green_tea': 1,\n",
       "             'hate': 1,\n",
       "             'high': 1,\n",
       "             'highlight': 1,\n",
       "             'hostess': 1,\n",
       "             'ice_cream': 1,\n",
       "             'iced': 1,\n",
       "             'kind_of': 1,\n",
       "             'know': 1,\n",
       "             'lead': 1,\n",
       "             'learn': 1,\n",
       "             'lgo': 2,\n",
       "             'line': 1,\n",
       "             'live': 1,\n",
       "             'look': 3,\n",
       "             'look_like': 1,\n",
       "             'mind': 1,\n",
       "             'minute': 1,\n",
       "             'more_often': 1,\n",
       "             'more_than': 1,\n",
       "             'mother': 1,\n",
       "             'occasional': 1,\n",
       "             'old_fashioned': 1,\n",
       "             'ooey': 2,\n",
       "             'order': 2,\n",
       "             'parking': 2,\n",
       "             'pastry': 2,\n",
       "             'people': 2,\n",
       "             'person': 1,\n",
       "             'pizza': 2,\n",
       "             'potatoe': 1,\n",
       "             'price': 1,\n",
       "             'red_velvet': 1,\n",
       "             'resist': 1,\n",
       "             'rest_of': 1,\n",
       "             'restaurant': 1,\n",
       "             'review': 1,\n",
       "             'sandwich': 2,\n",
       "             'shelf': 1,\n",
       "             'shop': 1,\n",
       "             'snobby': 1,\n",
       "             'south': 1,\n",
       "             'splendid': 1,\n",
       "             'spot': 2,\n",
       "             'street': 1,\n",
       "             'sure': 1,\n",
       "             'sushi_roll': 1,\n",
       "             'sweet': 1,\n",
       "             'tammie': 1,\n",
       "             'taste': 1,\n",
       "             'trust': 1,\n",
       "             'turkey': 1,\n",
       "             'vacate': 1,\n",
       "             'veg': 1,\n",
       "             'vegan': 1,\n",
       "             'vegetarian': 2,\n",
       "             'wait': 2,\n",
       "             'wonderful': 1,\n",
       "             'worker': 1})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[defaultdict(int,\n",
       "             {\"'s\": 3,\n",
       "              '-PRON-': 26,\n",
       "              '7th': 1,\n",
       "              'ask': 1,\n",
       "              'ave': 1,\n",
       "              'barber': 1,\n",
       "              'barber_shop': 2,\n",
       "              'barely': 1,\n",
       "              'black': 2,\n",
       "              'close': 1,\n",
       "              'come': 1,\n",
       "              'come_back': 1,\n",
       "              'come_here': 1,\n",
       "              'conclusion': 1,\n",
       "              'cut': 2,\n",
       "              'do_not': 1,\n",
       "              'experience': 2,\n",
       "              'extra': 1,\n",
       "              'fade': 1,\n",
       "              'go_back': 1,\n",
       "              'guy': 3,\n",
       "              'hair': 1,\n",
       "              'haircut': 2,\n",
       "              'hanlon': 1,\n",
       "              'head': 1,\n",
       "              'heavyset': 1,\n",
       "              'imply': 1,\n",
       "              'invoke': 1,\n",
       "              'kind_of': 1,\n",
       "              'left': 1,\n",
       "              'length': 1,\n",
       "              'lest': 1,\n",
       "              'likely': 1,\n",
       "              'little': 1,\n",
       "              'loyal_customer': 1,\n",
       "              'malice': 1,\n",
       "              'man': 1,\n",
       "              'nate': 1,\n",
       "              'occasion': 1,\n",
       "              'place': 3,\n",
       "              'presume': 2,\n",
       "              'racism': 1,\n",
       "              'razor': 1,\n",
       "              'real': 1,\n",
       "              'regular': 2,\n",
       "              'remind': 1,\n",
       "              'second': 2,\n",
       "              'shape': 1,\n",
       "              'simple': 2,\n",
       "              'special': 1,\n",
       "              'stay': 1,\n",
       "              'steve': 1,\n",
       "              'stupidity': 1,\n",
       "              'style': 1,\n",
       "              'tell': 1,\n",
       "              'thank': 1,\n",
       "              'think': 1,\n",
       "              'tomorrow': 1,\n",
       "              'walk_in': 1,\n",
       "              'want': 2,\n",
       "              'way': 1,\n",
       "              'will_never': 1,\n",
       "              'worth': 1,\n",
       "              'zero_star': 1}),\n",
       " defaultdict(int,\n",
       "             {'-PRON-': 5,\n",
       "              'business': 1,\n",
       "              'know': 1,\n",
       "              'love': 1,\n",
       "              'nice': 1,\n",
       "              'outstanding': 1,\n",
       "              'peep': 1,\n",
       "              'service': 1,\n",
       "              'starbucks': 1,\n",
       "              'sure': 1,\n",
       "              'work': 1}),\n",
       " defaultdict(int,\n",
       "             {'-PRON-': 7,\n",
       "              '1000': 1,\n",
       "              'as_well': 1,\n",
       "              'atmosphere': 1,\n",
       "              'attentive': 1,\n",
       "              'bartender': 1,\n",
       "              'big': 1,\n",
       "              'brickyard': 1,\n",
       "              'chandler': 1,\n",
       "              'cocktail': 2,\n",
       "              'come': 1,\n",
       "              'conversation': 1,\n",
       "              'corpses': 1,\n",
       "              'downtown': 1,\n",
       "              'easy': 1,\n",
       "              'good': 3,\n",
       "              'great': 2,\n",
       "              'house': 1,\n",
       "              'know': 1,\n",
       "              'light': 1,\n",
       "              'music': 1,\n",
       "              'overall': 1,\n",
       "              'overbearing': 1,\n",
       "              'rave_about': 1,\n",
       "              'revivers': 1,\n",
       "              'second_time': 1,\n",
       "              'smooth': 1,\n",
       "              'spot': 1,\n",
       "              'swamp': 1,\n",
       "              'thang': 1,\n",
       "              'too_loud': 1}),\n",
       " defaultdict(int,\n",
       "             {'-PRON-': 21,\n",
       "              'access': 1,\n",
       "              'alright': 1,\n",
       "              'appreciate': 1,\n",
       "              'astonishingly': 1,\n",
       "              'audience': 1,\n",
       "              'babble': 1,\n",
       "              'bad': 1,\n",
       "              'captive': 1,\n",
       "              'chain': 1,\n",
       "              'coffee': 2,\n",
       "              'come': 2,\n",
       "              'connection': 1,\n",
       "              'conversation': 1,\n",
       "              'do_not': 2,\n",
       "              'enjoyable': 1,\n",
       "              'farm': 1,\n",
       "              'find': 1,\n",
       "              'free': 1,\n",
       "              'free_wifi': 1,\n",
       "              'frequently': 1,\n",
       "              'giant': 1,\n",
       "              'girl': 2,\n",
       "              'good': 1,\n",
       "              'guess': 1,\n",
       "              'have_no_idea': 1,\n",
       "              'high_school': 2,\n",
       "              'hour': 1,\n",
       "              'inane': 1,\n",
       "              'inexpensive': 1,\n",
       "              'internet': 2,\n",
       "              'life': 1,\n",
       "              'live': 1,\n",
       "              'many_people': 1,\n",
       "              'monthly': 1,\n",
       "              'moronic': 1,\n",
       "              'naturally': 1,\n",
       "              'pay': 2,\n",
       "              'people_who': 1,\n",
       "              'place': 2,\n",
       "              'pointless': 1,\n",
       "              'pretty': 1,\n",
       "              'realization': 1,\n",
       "              'scottsdale': 1,\n",
       "              'staff': 1,\n",
       "              'stick': 1,\n",
       "              'subscription': 1,\n",
       "              'suppose': 1,\n",
       "              'talk': 1,\n",
       "              'teenager': 1,\n",
       "              'thing': 2,\n",
       "              'this_place': 1,\n",
       "              'topic': 1,\n",
       "              'town': 1,\n",
       "              'treat': 1,\n",
       "              'trophy': 1,\n",
       "              'untrained': 1,\n",
       "              'use': 1,\n",
       "              'value': 1,\n",
       "              'wife': 1,\n",
       "              'wired': 1,\n",
       "              'worth': 1}),\n",
       " defaultdict(int,\n",
       "             {'-PRON-': 10,\n",
       "              'add': 1,\n",
       "              'appetite': 1,\n",
       "              'bacon': 1,\n",
       "              'big': 1,\n",
       "              'bite': 1,\n",
       "              'bomb': 1,\n",
       "              'burger': 1,\n",
       "              'can_not': 1,\n",
       "              'corndog': 1,\n",
       "              'couple': 2,\n",
       "              'crispy': 1,\n",
       "              'da': 1,\n",
       "              'delicious': 1,\n",
       "              'eat_here': 1,\n",
       "              'fantastic': 1,\n",
       "              'fries': 1,\n",
       "              'fry': 1,\n",
       "              'go_back': 1,\n",
       "              'grill_cheese': 1,\n",
       "              'homemade': 1,\n",
       "              'last_time': 1,\n",
       "              'list': 1,\n",
       "              'melt': 1,\n",
       "              'of_course': 1,\n",
       "              'opportunity': 1,\n",
       "              'order': 1,\n",
       "              'pimento': 1,\n",
       "              'pimento_cheese': 1,\n",
       "              'pimp': 1,\n",
       "              \"pimp'n\": 1,\n",
       "              'place': 1,\n",
       "              'ranch': 1,\n",
       "              'season': 1,\n",
       "              'sound': 1,\n",
       "              'time': 1,\n",
       "              'waffle': 1,\n",
       "              'wait': 1}),\n",
       " defaultdict(int,\n",
       "             {'$': 1,\n",
       "              '-PRON-': 1,\n",
       "              '6.95': 1,\n",
       "              'choice': 2,\n",
       "              'come': 1,\n",
       "              'come_back': 1,\n",
       "              'consistent': 1,\n",
       "              'entree': 1,\n",
       "              'food': 2,\n",
       "              'fresh': 1,\n",
       "              'good': 1,\n",
       "              'hot': 1,\n",
       "              'lunch_special': 1,\n",
       "              'order': 1,\n",
       "              'rice': 1,\n",
       "              'salad': 1,\n",
       "              'soup': 1,\n",
       "              'thai': 1}),\n",
       " defaultdict(int,\n",
       "             {'-PRON-': 7,\n",
       "              'absolutely_love': 1,\n",
       "              'add': 1,\n",
       "              'bar': 1,\n",
       "              'basket': 1,\n",
       "              'beer': 1,\n",
       "              'burger': 2,\n",
       "              'compare': 1,\n",
       "              'cook': 1,\n",
       "              'delux': 4,\n",
       "              'enjoy': 1,\n",
       "              'especially': 1,\n",
       "              'flight': 1,\n",
       "              'forget': 1,\n",
       "              'heaven': 1,\n",
       "              'medium': 1,\n",
       "              'offer': 1,\n",
       "              'order': 2,\n",
       "              'place': 1,\n",
       "              'sauce': 1,\n",
       "              'sit_at': 1,\n",
       "              'southwestern': 1,\n",
       "              'splitting': 1,\n",
       "              'style': 1,\n",
       "              'sunbru': 1,\n",
       "              'sweet_potato_fry': 2,\n",
       "              'uncommon': 1,\n",
       "              'wine': 1}),\n",
       " defaultdict(int,\n",
       "             {'-PRON-': 8,\n",
       "              'budd': 2,\n",
       "              'care': 1,\n",
       "              'centrally_locate': 1,\n",
       "              'dr': 1,\n",
       "              'easy': 1,\n",
       "              'fair': 1,\n",
       "              'find': 1,\n",
       "              'fun': 1,\n",
       "              'giveaway': 1,\n",
       "              'good': 1,\n",
       "              'kid': 1,\n",
       "              'kind': 1,\n",
       "              'location': 1,\n",
       "              'orthodontic': 1,\n",
       "              'patient': 1,\n",
       "              'payment': 1,\n",
       "              'plan': 1,\n",
       "              'pricing': 1,\n",
       "              'provide': 1,\n",
       "              'regard': 1,\n",
       "              'regret': 1,\n",
       "              'smile': 1,\n",
       "              'team': 3,\n",
       "              'welcome': 1,\n",
       "              'will_never': 1,\n",
       "              'win': 1,\n",
       "              'work': 1}),\n",
       " defaultdict(int,\n",
       "             {'area': 1,\n",
       "              'cal': 1,\n",
       "              'calamari': 1,\n",
       "              'conveniently_locate': 1,\n",
       "              'excellent': 1,\n",
       "              'experience': 1,\n",
       "              'fremont': 1,\n",
       "              'italia': 1,\n",
       "              'pizza': 1,\n",
       "              'sat': 1,\n",
       "              'sport_bar': 1,\n",
       "              'street': 1}),\n",
       " defaultdict(int,\n",
       "             {'-PRON-': 34,\n",
       "              'add': 1,\n",
       "              'anyways': 1,\n",
       "              'appreciate': 1,\n",
       "              'arjuna': 1,\n",
       "              'artisan': 1,\n",
       "              'available': 1,\n",
       "              'awesome': 1,\n",
       "              'basket': 3,\n",
       "              'bread': 1,\n",
       "              'chance': 1,\n",
       "              'chocolate_cake': 1,\n",
       "              'company': 1,\n",
       "              'customize': 1,\n",
       "              'damage': 1,\n",
       "              'deliver': 1,\n",
       "              'delivery': 2,\n",
       "              'disappoint': 1,\n",
       "              'drop': 1,\n",
       "              'eat': 1,\n",
       "              'especially': 1,\n",
       "              'farm': 1,\n",
       "              'farms': 1,\n",
       "              'fresh': 1,\n",
       "              'great': 1,\n",
       "              'grocery_store': 1,\n",
       "              'grow': 1,\n",
       "              'happen': 1,\n",
       "              'happy': 1,\n",
       "              'harvest': 1,\n",
       "              'home': 2,\n",
       "              'jam': 1,\n",
       "              'late': 1,\n",
       "              'like': 3,\n",
       "              'limited': 1,\n",
       "              'local': 2,\n",
       "              'location': 1,\n",
       "              'lufa': 3,\n",
       "              'meat': 1,\n",
       "              'member': 1,\n",
       "              'montreal': 1,\n",
       "              'nearby': 1,\n",
       "              'notification': 1,\n",
       "              'offer': 4,\n",
       "              'organic': 1,\n",
       "              'pack': 1,\n",
       "              'partner': 1,\n",
       "              'pick': 1,\n",
       "              'pick_up': 1,\n",
       "              'plan_ahead': 1,\n",
       "              'point': 1,\n",
       "              'product': 6,\n",
       "              'pumpkin': 1,\n",
       "              'quality': 1,\n",
       "              'receive': 1,\n",
       "              'reimbursement': 1,\n",
       "              'rooftop': 1,\n",
       "              'service': 3,\n",
       "              'sew': 1,\n",
       "              'squish': 1,\n",
       "              'sustainable': 2,\n",
       "              'sweet': 1,\n",
       "              'thank': 1,\n",
       "              'try': 1,\n",
       "              'variety_of': 1,\n",
       "              'veggie': 1,\n",
       "              'visit': 3,\n",
       "              'voila': 1,\n",
       "              'week': 1,\n",
       "              'weekly': 1,\n",
       "              'work': 1,\n",
       "              'year': 1})]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://programminghistorian.org/en/lessons/counting-frequencies\n",
    "dict_series[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordListToFreqDict(wordlist):\n",
    "    wordfreq = [wordlist.count(p) for p in wordlist]\n",
    "    return dict(zip(wordlist,wordfreq))\n",
    "\n",
    "review_series = [wordListToFreqDict(wordstring.split()) for wordstring in lines_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{\"'s\": 3,\n",
       "  '-PRON-': 26,\n",
       "  '7th': 1,\n",
       "  'ask': 1,\n",
       "  'ave': 1,\n",
       "  'barber': 1,\n",
       "  'barber_shop': 2,\n",
       "  'barely': 1,\n",
       "  'black': 2,\n",
       "  'close': 1,\n",
       "  'come': 1,\n",
       "  'come_back': 1,\n",
       "  'come_here': 1,\n",
       "  'conclusion': 1,\n",
       "  'cut': 2,\n",
       "  'do_not': 1,\n",
       "  'experience': 2,\n",
       "  'extra': 1,\n",
       "  'fade': 1,\n",
       "  'go_back': 1,\n",
       "  'guy': 3,\n",
       "  'hair': 1,\n",
       "  'haircut': 2,\n",
       "  'hanlon': 1,\n",
       "  'head': 1,\n",
       "  'heavyset': 1,\n",
       "  'imply': 1,\n",
       "  'invoke': 1,\n",
       "  'kind_of': 1,\n",
       "  'left': 1,\n",
       "  'length': 1,\n",
       "  'lest': 1,\n",
       "  'likely': 1,\n",
       "  'little': 1,\n",
       "  'loyal_customer': 1,\n",
       "  'malice': 1,\n",
       "  'man': 1,\n",
       "  'nate': 1,\n",
       "  'occasion': 1,\n",
       "  'place': 3,\n",
       "  'presume': 2,\n",
       "  'racism': 1,\n",
       "  'razor': 1,\n",
       "  'real': 1,\n",
       "  'regular': 2,\n",
       "  'remind': 1,\n",
       "  'second': 2,\n",
       "  'shape': 1,\n",
       "  'simple': 2,\n",
       "  'special': 1,\n",
       "  'stay': 1,\n",
       "  'steve': 1,\n",
       "  'stupidity': 1,\n",
       "  'style': 1,\n",
       "  'tell': 1,\n",
       "  'thank': 1,\n",
       "  'think': 1,\n",
       "  'tomorrow': 1,\n",
       "  'walk_in': 1,\n",
       "  'want': 2,\n",
       "  'way': 1,\n",
       "  'will_never': 1,\n",
       "  'worth': 1,\n",
       "  'zero_star': 1},\n",
       " {'-PRON-': 5,\n",
       "  'business': 1,\n",
       "  'know': 1,\n",
       "  'love': 1,\n",
       "  'nice': 1,\n",
       "  'outstanding': 1,\n",
       "  'peep': 1,\n",
       "  'service': 1,\n",
       "  'starbucks': 1,\n",
       "  'sure': 1,\n",
       "  'work': 1},\n",
       " {'-PRON-': 7,\n",
       "  '1000': 1,\n",
       "  'as_well': 1,\n",
       "  'atmosphere': 1,\n",
       "  'attentive': 1,\n",
       "  'bartender': 1,\n",
       "  'big': 1,\n",
       "  'brickyard': 1,\n",
       "  'chandler': 1,\n",
       "  'cocktail': 2,\n",
       "  'come': 1,\n",
       "  'conversation': 1,\n",
       "  'corpses': 1,\n",
       "  'downtown': 1,\n",
       "  'easy': 1,\n",
       "  'good': 3,\n",
       "  'great': 2,\n",
       "  'house': 1,\n",
       "  'know': 1,\n",
       "  'light': 1,\n",
       "  'music': 1,\n",
       "  'overall': 1,\n",
       "  'overbearing': 1,\n",
       "  'rave_about': 1,\n",
       "  'revivers': 1,\n",
       "  'second_time': 1,\n",
       "  'smooth': 1,\n",
       "  'spot': 1,\n",
       "  'swamp': 1,\n",
       "  'thang': 1,\n",
       "  'too_loud': 1},\n",
       " {'-PRON-': 21,\n",
       "  'access': 1,\n",
       "  'alright': 1,\n",
       "  'appreciate': 1,\n",
       "  'astonishingly': 1,\n",
       "  'audience': 1,\n",
       "  'babble': 1,\n",
       "  'bad': 1,\n",
       "  'captive': 1,\n",
       "  'chain': 1,\n",
       "  'coffee': 2,\n",
       "  'come': 2,\n",
       "  'connection': 1,\n",
       "  'conversation': 1,\n",
       "  'do_not': 2,\n",
       "  'enjoyable': 1,\n",
       "  'farm': 1,\n",
       "  'find': 1,\n",
       "  'free': 1,\n",
       "  'free_wifi': 1,\n",
       "  'frequently': 1,\n",
       "  'giant': 1,\n",
       "  'girl': 2,\n",
       "  'good': 1,\n",
       "  'guess': 1,\n",
       "  'have_no_idea': 1,\n",
       "  'high_school': 2,\n",
       "  'hour': 1,\n",
       "  'inane': 1,\n",
       "  'inexpensive': 1,\n",
       "  'internet': 2,\n",
       "  'life': 1,\n",
       "  'live': 1,\n",
       "  'many_people': 1,\n",
       "  'monthly': 1,\n",
       "  'moronic': 1,\n",
       "  'naturally': 1,\n",
       "  'pay': 2,\n",
       "  'people_who': 1,\n",
       "  'place': 2,\n",
       "  'pointless': 1,\n",
       "  'pretty': 1,\n",
       "  'realization': 1,\n",
       "  'scottsdale': 1,\n",
       "  'staff': 1,\n",
       "  'stick': 1,\n",
       "  'subscription': 1,\n",
       "  'suppose': 1,\n",
       "  'talk': 1,\n",
       "  'teenager': 1,\n",
       "  'thing': 2,\n",
       "  'this_place': 1,\n",
       "  'topic': 1,\n",
       "  'town': 1,\n",
       "  'treat': 1,\n",
       "  'trophy': 1,\n",
       "  'untrained': 1,\n",
       "  'use': 1,\n",
       "  'value': 1,\n",
       "  'wife': 1,\n",
       "  'wired': 1,\n",
       "  'worth': 1},\n",
       " {'-PRON-': 10,\n",
       "  'add': 1,\n",
       "  'appetite': 1,\n",
       "  'bacon': 1,\n",
       "  'big': 1,\n",
       "  'bite': 1,\n",
       "  'bomb': 1,\n",
       "  'burger': 1,\n",
       "  'can_not': 1,\n",
       "  'corndog': 1,\n",
       "  'couple': 2,\n",
       "  'crispy': 1,\n",
       "  'da': 1,\n",
       "  'delicious': 1,\n",
       "  'eat_here': 1,\n",
       "  'fantastic': 1,\n",
       "  'fries': 1,\n",
       "  'fry': 1,\n",
       "  'go_back': 1,\n",
       "  'grill_cheese': 1,\n",
       "  'homemade': 1,\n",
       "  'last_time': 1,\n",
       "  'list': 1,\n",
       "  'melt': 1,\n",
       "  'of_course': 1,\n",
       "  'opportunity': 1,\n",
       "  'order': 1,\n",
       "  'pimento': 1,\n",
       "  'pimento_cheese': 1,\n",
       "  'pimp': 1,\n",
       "  \"pimp'n\": 1,\n",
       "  'place': 1,\n",
       "  'ranch': 1,\n",
       "  'season': 1,\n",
       "  'sound': 1,\n",
       "  'time': 1,\n",
       "  'waffle': 1,\n",
       "  'wait': 1}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_series[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find a way to remove words which are not specific to restaurants\n",
    "# idea: pick a business category which has very less similarity to \n",
    "        # restaurants and remove those words from our word lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating businesses by combined reviews will help in identifying similar restaurants.\n",
    "    # can use this for recommending new restaurants\n",
    "    # can also use this in identifying similar users (i.e similar restaurants liked by diff users)\n",
    "# Aggregating user's by combining reviews will help in identifying similar restaurants.\n",
    "    # we will get some sort of score\n",
    "# probably could use that for users\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finally\n"
     ]
    }
   ],
   "source": [
    "### Spell checking\n",
    "### english words has a max of two repeated letters consecutively\n",
    "def reduce_lengthening(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "print(reduce_lengthening( \"finallllllly\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (tree.py, line 37)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2910\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-55-1e7b493cf37b>\"\u001b[0m, line \u001b[1;32m6\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from pattern3.en import spelling, suggest\n",
      "  File \u001b[1;32m\"/anaconda3/lib/python3.6/site-packages/pattern3/text/en/__init__.py\"\u001b[0m, line \u001b[1;32m22\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from pattern3.text import (\n",
      "\u001b[0;36m  File \u001b[0;32m\"/anaconda3/lib/python3.6/site-packages/pattern3/text/__init__.py\"\u001b[0;36m, line \u001b[0;32m28\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from pattern3.text.tree import Tree, Text, Sentence, Slice, Chunk, PNPChunk, Chink, Word, table\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/anaconda3/lib/python3.6/site-packages/pattern3/text/tree.py\"\u001b[0;36m, line \u001b[0;32m37\u001b[0m\n\u001b[0;31m    except:\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "#MODULE = '/Users/rajeshpothamsetty/Downloads/pattern-master/pattern'\n",
    "#import sys\n",
    "#if MODULE not in sys.path: sys.path.append(MODULE)\n",
    "#import pattern\n",
    "\n",
    "from pattern3.en import spelling, suggest\n",
    "\n",
    "word = \"amazzziiing\"\n",
    "word_wlf = reduce_lengthening(word) #calling function defined above\n",
    "print(word_wlf) #word lengthening isn't being able to fix it completely\n",
    "\n",
    "correct_word = suggest(word_wlf) \n",
    "print(correct_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "\n",
    "temp_textblob = TextBlob(x_temp)\n",
    "#print(temp_textblob.correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_correct = temp_textblob.correct()\n",
    "temp_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'temp_correct' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-8921e2034f92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_correct\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#removes 'e' from arrive; why?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'temp_correct' is not defined"
     ]
    }
   ],
   "source": [
    "clean_text(temp_correct) #removes 'e' from arrive; why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"reserve\")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob('reserv').correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"arrive\")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob('arriv').correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pyspark Basics\n",
    "## https://www.dezyre.com/apache-spark-tutorial/pyspark-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "from pyspark.ml.feature import RegexTokenizer, CountVectorizer\n",
    "from pyspark.ml.feature import StopWordsRemover, VectorAssembler\n",
    "from pyspark.ml.feature import Word2Vec, Word2VecModel\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import folium\n",
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Volumes/Transcend/dataset/'\n",
    "model_path = '/Volumes/Transcend/MDS_Yelp/model/'\n",
    "outout_path = '/Volumes/Transcend/MDS_Yelp/output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- business_name: string (nullable = true)\n",
      " |-- neighborhood: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- postal_code: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- review_count: long (nullable = true)\n",
      " |-- categories: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "business_df = spark.read.parquet(data_path + 'business-small.parquet')\n",
    "business_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- review_count: long (nullable = true)\n",
      " |-- yelping_since: string (nullable = true)\n",
      " |-- useful: long (nullable = true)\n",
      " |-- funny: long (nullable = true)\n",
      " |-- cool: long (nullable = true)\n",
      " |-- fans: long (nullable = true)\n",
      " |-- average_stars: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df = spark.read.parquet(data_path + 'user-small.parquet')\n",
    "user_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- stars: long (nullable = true)\n",
      " |-- review_date: string (nullable = true)\n",
      " |-- review_text: string (nullable = true)\n",
      " |-- useful: long (nullable = true)\n",
      " |-- funny: long (nullable = true)\n",
      " |-- cool: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_df = spark.read.parquet(data_path + 'review-small.parquet')\n",
    "review_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             user_id|         review_text|\n",
      "+--------------------+--------------------+\n",
      "|u0LXt3Uea_GidxRW1...|Who would have gu...|\n",
      "|u0LXt3Uea_GidxRW1...|Not bad!! Love th...|\n",
      "|u0LXt3Uea_GidxRW1...|This is currently...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create SQL view for later queries\n",
    "review_df.createOrReplaceTempView(\"reviews\")\n",
    "\n",
    "# create review text dataframe\n",
    "reviews_text = spark.sql(\"SELECT user_id, review_text FROM reviews\")\n",
    "reviews_text.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73041"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate all reviews per restuarant\n",
    "\n",
    "reviews_text_rdd = reviews_text.rdd\n",
    "reviews_by_user_rdd = reviews_text_rdd.map(tuple).reduceByKey(add)  \n",
    "reviews_by_user_df = spark.createDataFrame(reviews_by_user_rdd)\n",
    "reviews_by_user_df = reviews_by_user_df \\\n",
    "                            .withColumnRenamed('_1', 'user_id') \\\n",
    "                            .withColumnRenamed('_2', 'text')\n",
    "reviews_by_user_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_by_user_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_by_user_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [Hi, I, heard, about, Spark] => \n",
      "Vector: [-0.011087720049545169,-0.04713543206453324,-0.030747681856155396]\n",
      "\n",
      "Text: [I, wish, Java, could, use, case, classes] => \n",
      "Vector: [0.02089989078896386,0.025788720164980204,-0.029980793063129695]\n",
      "\n",
      "Text: [Logistic, regression, models, are, neat] => \n",
      "Vector: [0.03326573260128498,0.010403224825859071,-0.04281422272324562]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Example of using Word2vec\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "# Input data: Each row is a bag of words from a sentence or document.\n",
    "documentDF = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "\n",
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "\n",
    "result = model.transform(documentDF)\n",
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.Word2Vec\n",
    "\n",
    "## The minimum number of times a token must appear to be included in the word2vec model's vocabulary\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning of vectorSize and minCount required in word2vec model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 123 ms, sys: 37.3 ms, total: 160 ms\n",
      "Wall time: 9min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create text processing pipeline -- this a lengthy resource-intensive process\n",
    "# Build the pipeline \n",
    "regexTokenizer = RegexTokenizer(gaps = False, pattern = '\\w+', inputCol = 'text', outputCol = 'token')\n",
    "stopWordsRemover = StopWordsRemover(inputCol = 'token', outputCol = 'nostopwrd')\n",
    "countVectorizer = CountVectorizer(inputCol=\"nostopwrd\", outputCol=\"rawFeature\")\n",
    "iDF = IDF(inputCol=\"rawFeature\", outputCol=\"idf_vec\")\n",
    "word2Vec = Word2Vec(vectorSize = 100, minCount = 5, inputCol = 'nostopwrd', outputCol = 'word_vec', seed=123)\n",
    "vectorAssembler = VectorAssembler(inputCols=['idf_vec', 'word_vec'], outputCol='comb_vec')\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopWordsRemover, countVectorizer, iDF, word2Vec, vectorAssembler])\n",
    "\n",
    "# fit the model\n",
    "pipeline_mdl = pipeline.fit(reviews_by_user_df)\n",
    "\n",
    "#save the pipeline model\n",
    "pipeline_mdl.write().overwrite().save(model_path + 'pipe_txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the text transformation pipeline trained model\n",
    "pipeline_mdl = PipelineModel.load(model_path + 'pipe_txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the review data\n",
    "reviews_by_user_trf_df = pipeline_mdl.transform(reviews_by_user_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|           nostopwrd|             idf_vec|            word_vec|            comb_vec|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|I give up on this...|[give, place, got...|(128365,[0,1,2,3,...|[-0.0374168294159...|(128465,[0,1,2,3,...|\n",
      "|Love this place m...|[love, place, mak...|(128365,[0,1,2,3,...|[-0.0216792637887...|(128465,[0,1,2,3,...|\n",
      "|Phenomenal brunch...|[phenomenal, brun...|(128365,[0,1,2,3,...|[-0.0473526970104...|(128465,[0,1,2,3,...|\n",
      "|A born and bred T...|[born, bred, toro...|(128365,[0,1,2,3,...|[-0.0298230579082...|(128465,[0,1,2,3,...|\n",
      "|We really enjoyed...|[really, enjoyed,...|(128365,[0,1,2,3,...|[-0.0115986177697...|(128465,[0,1,2,3,...|\n",
      "|I was pretty unim...|[pretty, unimpres...|(128365,[0,1,2,3,...|[-0.0381316097274...|(128465,[0,1,2,3,...|\n",
      "|I have never done...|[never, done, one...|(128365,[2,6,11,1...|[-0.0256846508636...|(128465,[2,6,11,1...|\n",
      "|Seven Lives serve...|[seven, lives, se...|(128365,[0,1,2,3,...|[-0.0140019047037...|(128465,[0,1,2,3,...|\n",
      "|This is the best ...|[best, indian, fo...|(128365,[0,15,24,...|[-0.0584768957037...|(128465,[0,15,24,...|\n",
      "|Good ramen but ha...|[good, ramen, bet...|(128365,[1,9,10,1...|[0.07983644855474...|(128465,[1,9,10,1...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the transformed review data\n",
    "reviews_by_user_trf_df.select( 'text', 'nostopwrd', 'idf_vec', 'word_vec', 'comb_vec').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_by_user_trf_df.select( 'nostopwrd').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- token: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- nostopwrd: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- rawFeature: vector (nullable = true)\n",
      " |-- idf_vec: vector (nullable = true)\n",
      " |-- word_vec: vector (nullable = true)\n",
      " |-- comb_vec: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews_by_user_trf_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'iDF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2a6fac126aed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'iDF' is not defined"
     ]
    }
   ],
   "source": [
    "type(iDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CosineSim(vec1, vec2): \n",
    "    return np.dot(vec1, vec2) / np.sqrt(np.dot(vec1, vec1)) / np.sqrt(np.dot(vec2, vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_user_vecs = reviews_by_user_trf_df.select('user_id', 'word_vec').rdd.map(lambda x: (x[0], x[1])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8652645262685584"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CosineSim(all_user_vecs[0][1], all_user_vecs[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('nOTl4aPC4tKHK35T3bNauQ',\n",
       " DenseVector([-0.0374, -0.0827, -0.0607, 0.0706, 0.0262, -0.0358, -0.0004, -0.009, -0.0103, -0.0207, -0.0211, -0.0439, -0.0347, 0.0174, -0.0089, -0.0437, -0.0669, -0.0098, -0.0301, -0.0068, -0.0761, -0.0492, 0.0009, 0.0052, 0.0626, -0.0469, -0.0064, -0.0302, -0.0117, 0.0336, 0.0578, 0.0054, 0.0243, -0.0307, -0.0272, -0.0705, 0.0969, 0.0265, -0.0284, -0.0408, -0.0362, -0.0576, -0.0311, 0.0778, 0.0081, -0.0428, -0.048, 0.0279, -0.0118, 0.014, 0.0095, -0.0122, -0.0123, -0.0328, -0.0584, 0.0126, 0.0096, 0.0456, 0.0413, 0.0209, -0.0138, -0.0004, 0.0237, -0.0566, -0.0347, 0.0297, 0.0447, 0.0419, -0.0252, 0.0758, 0.0014, -0.0045, 0.0094, 0.0375, 0.0088, -0.0188, -0.0655, 0.0098, -0.0166, 0.0423, 0.001, -0.0137, -0.0024, -0.0112, -0.0131, -0.0028, 0.0135, 0.0187, -0.0107, 0.0014, -0.0016, -0.0062, -0.0262, 0.0269, -0.0136, -0.0092, -0.0441, 0.0108, -0.0502, 0.025]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_user_vecs[0]\n",
    "# user_id = 'nOTl4aPC4tKHK35T3bNauQ'\n",
    "# DenseVector() - vector respresentation of all the reviews of User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_user_vecs[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do consider only if the USer had minm. number of reviews\n",
    "# Tuning VectorSize and minCount\n",
    "# maybe removing non-restaurant related words..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimilarUsers(b_ids, sim_user_limit=10):\n",
    "    \n",
    "    schema = StructType([   \n",
    "                            StructField(\"user_id\", StringType(), True)\n",
    "                            ,StructField(\"score\", IntegerType(), True)\n",
    "                            ,StructField(\"input_user_id\", StringType(), True)\n",
    "                        ])\n",
    "    \n",
    "    similar_user_df = spark.createDataFrame([], schema)\n",
    "    similar_user_df_all = spark.createDataFrame([], schema)\n",
    "    \n",
    "    for b_id in b_ids:\n",
    "        \n",
    "        input_vec = [(r[1]) for r in all_user_vecs if r[0] == b_id][0]\n",
    "        \n",
    "        similar_user_rdd = sc.parallelize((i[0], float(CosineSim(input_vec, i[1]))) for i in all_user_vecs)\n",
    "        \n",
    "        similar_user_df = spark.createDataFrame(similar_user_rdd) \\\n",
    "            .withColumnRenamed('_1', 'user_id') \\\n",
    "            .withColumnRenamed('_2', 'score') \\\n",
    "            .orderBy(\"score\", ascending = False)\n",
    "            \n",
    "        similar_user_df = similar_user_df.dropna()    \n",
    "        similar_user_df = similar_user_df.filter(col(\"user_id\") != b_id).limit(sim_user_limit)\n",
    "        similar_user_df = similar_user_df.withColumn('input_user_id', lit(b_id))\n",
    "        \n",
    "        similar_user_df = similar_user_df \\\n",
    "                                    .union(similar_user_df)\n",
    "        \n",
    "        similar_user_df_all = similar_user_df_all.union(similar_user_df)\n",
    "    similar_user_df_all = similar_user_df_all.dropDuplicates()    \n",
    "    return similar_user_df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUserDetails(in_bus):\n",
    "    \n",
    "    a = in_bus.alias(\"a\")\n",
    "    b = user_df.alias(\"b\")\n",
    "    \n",
    "    return a.join(b, col(\"a.user_id\") == col(\"b.user_id\"), 'inner') \\\n",
    "             .select([col('a.'+xx) for xx in a.columns] + [col('b.user_id'), col('b.user_name'),col('b.review_count')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input user details:\n",
      "+----------------------+---------+------------+\n",
      "|user_id               |user_name|review_count|\n",
      "+----------------------+---------+------------+\n",
      "|nOTl4aPC4tKHK35T3bNauQ|Katherine|148         |\n",
      "|QBac9-Ii6jR-yLsQ5MVTHg|Alex     |13          |\n",
      "+----------------------+---------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 similar Users for each input restaurant are:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_user_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>score</th>\n",
       "      <th>review_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>QBac9-Ii6jR-yLsQ5MVTHg</td>\n",
       "      <td>J5Eb7LhJaOa20k0ppcOCOg</td>\n",
       "      <td>Alek</td>\n",
       "      <td>0.907664</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nOTl4aPC4tKHK35T3bNauQ</td>\n",
       "      <td>ZBllYKrFzaI0I7v6Wl26Wg</td>\n",
       "      <td>Cecilia</td>\n",
       "      <td>0.963999</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QBac9-Ii6jR-yLsQ5MVTHg</td>\n",
       "      <td>cNhHuEQMIpLH_qc9qGz67A</td>\n",
       "      <td>Jay</td>\n",
       "      <td>0.910774</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QBac9-Ii6jR-yLsQ5MVTHg</td>\n",
       "      <td>_IR48ok0ZkPMWJ2PlRCk0A</td>\n",
       "      <td>Michael</td>\n",
       "      <td>0.907098</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>QBac9-Ii6jR-yLsQ5MVTHg</td>\n",
       "      <td>MpN81tQOL86GaFse-_tTRQ</td>\n",
       "      <td>Amy</td>\n",
       "      <td>0.913263</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>QBac9-Ii6jR-yLsQ5MVTHg</td>\n",
       "      <td>kw-YtOKPXrRB2a9wRZlmzQ</td>\n",
       "      <td>Jimmy</td>\n",
       "      <td>0.915453</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nOTl4aPC4tKHK35T3bNauQ</td>\n",
       "      <td>uO1w3qNo21c1bVHHFTYW0w</td>\n",
       "      <td>Joanne</td>\n",
       "      <td>0.972255</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nOTl4aPC4tKHK35T3bNauQ</td>\n",
       "      <td>v1nRL_f2EQs7fjh_m0TdQQ</td>\n",
       "      <td>Bryan</td>\n",
       "      <td>0.969187</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>QBac9-Ii6jR-yLsQ5MVTHg</td>\n",
       "      <td>g5W7s0n19gvT1Ujy_ITJog</td>\n",
       "      <td>Bria</td>\n",
       "      <td>0.907846</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>QBac9-Ii6jR-yLsQ5MVTHg</td>\n",
       "      <td>bPUpO-bP6BmAGvSwPyDsng</td>\n",
       "      <td>Michael</td>\n",
       "      <td>0.905848</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>QBac9-Ii6jR-yLsQ5MVTHg</td>\n",
       "      <td>xcBcIMVWEx1p7V8ACaeHpQ</td>\n",
       "      <td>Rick</td>\n",
       "      <td>0.918668</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>QBac9-Ii6jR-yLsQ5MVTHg</td>\n",
       "      <td>zkKv10FEw-iWJBHvK0mGfA</td>\n",
       "      <td>Paul</td>\n",
       "      <td>0.903733</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>nOTl4aPC4tKHK35T3bNauQ</td>\n",
       "      <td>PGx4HvY5joEeqXzam6tO7A</td>\n",
       "      <td>Lisa</td>\n",
       "      <td>0.965808</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>nOTl4aPC4tKHK35T3bNauQ</td>\n",
       "      <td>VVm-TFCpi9M1-k8ED0l1eA</td>\n",
       "      <td>Juliana</td>\n",
       "      <td>0.971013</td>\n",
       "      <td>442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>nOTl4aPC4tKHK35T3bNauQ</td>\n",
       "      <td>yL60onlTh1BmJKVLli_K3g</td>\n",
       "      <td>Danielle</td>\n",
       "      <td>0.964604</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nOTl4aPC4tKHK35T3bNauQ</td>\n",
       "      <td>vJGLEHyhCs9V-5fAe-xx3w</td>\n",
       "      <td>Lisa</td>\n",
       "      <td>0.968622</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>nOTl4aPC4tKHK35T3bNauQ</td>\n",
       "      <td>n4usc2DF4pd9fdi2wLDloQ</td>\n",
       "      <td>Lidiya</td>\n",
       "      <td>0.964592</td>\n",
       "      <td>343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>nOTl4aPC4tKHK35T3bNauQ</td>\n",
       "      <td>myrcQ3h2G04Gv-ANG_oqrg</td>\n",
       "      <td>Linda</td>\n",
       "      <td>0.971381</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>nOTl4aPC4tKHK35T3bNauQ</td>\n",
       "      <td>USUw_VT31ygQ9JkEc6nPsg</td>\n",
       "      <td>NJ</td>\n",
       "      <td>0.965363</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>QBac9-Ii6jR-yLsQ5MVTHg</td>\n",
       "      <td>eV5usRjY2cDqNKVv8wXroA</td>\n",
       "      <td>Sam</td>\n",
       "      <td>0.905077</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             input_user_id                 user_id user_name     score  \\\n",
       "0   QBac9-Ii6jR-yLsQ5MVTHg  J5Eb7LhJaOa20k0ppcOCOg      Alek  0.907664   \n",
       "1   nOTl4aPC4tKHK35T3bNauQ  ZBllYKrFzaI0I7v6Wl26Wg   Cecilia  0.963999   \n",
       "2   QBac9-Ii6jR-yLsQ5MVTHg  cNhHuEQMIpLH_qc9qGz67A       Jay  0.910774   \n",
       "3   QBac9-Ii6jR-yLsQ5MVTHg  _IR48ok0ZkPMWJ2PlRCk0A   Michael  0.907098   \n",
       "4   QBac9-Ii6jR-yLsQ5MVTHg  MpN81tQOL86GaFse-_tTRQ       Amy  0.913263   \n",
       "5   QBac9-Ii6jR-yLsQ5MVTHg  kw-YtOKPXrRB2a9wRZlmzQ     Jimmy  0.915453   \n",
       "6   nOTl4aPC4tKHK35T3bNauQ  uO1w3qNo21c1bVHHFTYW0w    Joanne  0.972255   \n",
       "7   nOTl4aPC4tKHK35T3bNauQ  v1nRL_f2EQs7fjh_m0TdQQ     Bryan  0.969187   \n",
       "8   QBac9-Ii6jR-yLsQ5MVTHg  g5W7s0n19gvT1Ujy_ITJog      Bria  0.907846   \n",
       "9   QBac9-Ii6jR-yLsQ5MVTHg  bPUpO-bP6BmAGvSwPyDsng   Michael  0.905848   \n",
       "10  QBac9-Ii6jR-yLsQ5MVTHg  xcBcIMVWEx1p7V8ACaeHpQ      Rick  0.918668   \n",
       "11  QBac9-Ii6jR-yLsQ5MVTHg  zkKv10FEw-iWJBHvK0mGfA      Paul  0.903733   \n",
       "12  nOTl4aPC4tKHK35T3bNauQ  PGx4HvY5joEeqXzam6tO7A      Lisa  0.965808   \n",
       "13  nOTl4aPC4tKHK35T3bNauQ  VVm-TFCpi9M1-k8ED0l1eA   Juliana  0.971013   \n",
       "14  nOTl4aPC4tKHK35T3bNauQ  yL60onlTh1BmJKVLli_K3g  Danielle  0.964604   \n",
       "15  nOTl4aPC4tKHK35T3bNauQ  vJGLEHyhCs9V-5fAe-xx3w      Lisa  0.968622   \n",
       "16  nOTl4aPC4tKHK35T3bNauQ  n4usc2DF4pd9fdi2wLDloQ    Lidiya  0.964592   \n",
       "17  nOTl4aPC4tKHK35T3bNauQ  myrcQ3h2G04Gv-ANG_oqrg     Linda  0.971381   \n",
       "18  nOTl4aPC4tKHK35T3bNauQ  USUw_VT31ygQ9JkEc6nPsg        NJ  0.965363   \n",
       "19  QBac9-Ii6jR-yLsQ5MVTHg  eV5usRjY2cDqNKVv8wXroA       Sam  0.905077   \n",
       "\n",
       "    review_count  \n",
       "0             34  \n",
       "1            135  \n",
       "2             57  \n",
       "3             82  \n",
       "4             46  \n",
       "5            101  \n",
       "6            221  \n",
       "7             33  \n",
       "8             73  \n",
       "9            142  \n",
       "10            83  \n",
       "11            55  \n",
       "12           349  \n",
       "13           442  \n",
       "14           264  \n",
       "15           334  \n",
       "16           343  \n",
       "17           112  \n",
       "18            90  \n",
       "19           105  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test with two restaurants\n",
    "\n",
    "uids = ['nOTl4aPC4tKHK35T3bNauQ', 'QBac9-Ii6jR-yLsQ5MVTHg']\n",
    "\n",
    "print('\\ninput user details:')\n",
    "user_df.select('user_id','user_name', 'review_count') \\\n",
    "    .filter(user_df.user_id.isin(uids) == True).show(truncate=False)\n",
    "    \n",
    "# get top 10 similar business\n",
    "sim_users = getUserDetails(getSimilarUsers(uids))\n",
    "\n",
    "print('Top 10 similar Users for each input restaurant are:\"')\n",
    "sim_users.select('input_user_id', 'a.user_id', 'user_name', 'score','review_count').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getSimilarUsers(uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getSimilarUsers(uids).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([   \n",
    "                        StructField(\"user_id\", StringType(), True)\n",
    "                        ,StructField(\"score\", IntegerType(), True)\n",
    "                        ,StructField(\"input_user_id\", StringType(), True)\n",
    "                    ])\n",
    "\n",
    "similar_user_df = spark.createDataFrame([], schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nOTl4aPC4tKHK35T3bNauQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "[Row(user_id='uO1w3qNo21c1bVHHFTYW0w', score=0.9722554149044773, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='myrcQ3h2G04Gv-ANG_oqrg', score=0.971381229778663, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='VVm-TFCpi9M1-k8ED0l1eA', score=0.9710125785824026, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='v1nRL_f2EQs7fjh_m0TdQQ', score=0.969187261441371, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='vJGLEHyhCs9V-5fAe-xx3w', score=0.9686216169669871, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='PGx4HvY5joEeqXzam6tO7A', score=0.9658080239271861, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='USUw_VT31ygQ9JkEc6nPsg', score=0.9653626156916343, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='yL60onlTh1BmJKVLli_K3g', score=0.9646044155730369, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='n4usc2DF4pd9fdi2wLDloQ', score=0.9645918982625399, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='ZBllYKrFzaI0I7v6Wl26Wg', score=0.9639987177918352, input_user_id='nOTl4aPC4tKHK35T3bNauQ')]\n",
      "-------------------------------------\n",
      "QBac9-Ii6jR-yLsQ5MVTHg\n",
      "-------------------------------------\n",
      "[Row(user_id='uO1w3qNo21c1bVHHFTYW0w', score=0.9722554149044773, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='myrcQ3h2G04Gv-ANG_oqrg', score=0.971381229778663, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='VVm-TFCpi9M1-k8ED0l1eA', score=0.9710125785824026, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='v1nRL_f2EQs7fjh_m0TdQQ', score=0.969187261441371, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='vJGLEHyhCs9V-5fAe-xx3w', score=0.9686216169669871, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='PGx4HvY5joEeqXzam6tO7A', score=0.9658080239271861, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='USUw_VT31ygQ9JkEc6nPsg', score=0.9653626156916343, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='yL60onlTh1BmJKVLli_K3g', score=0.9646044155730369, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='n4usc2DF4pd9fdi2wLDloQ', score=0.9645918982625399, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='ZBllYKrFzaI0I7v6Wl26Wg', score=0.9639987177918352, input_user_id='nOTl4aPC4tKHK35T3bNauQ')]\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sim_user_limit = 10\n",
    "\n",
    "schema = StructType([StructField('user_id',StringType(),True),StructField('score',DoubleType(),True),StructField('input_user_id',StringType(),False)])\n",
    "#schema = StructType([])\n",
    "temp = sqlContext.createDataFrame(sc.emptyRDD(), schema)\n",
    "\n",
    "for b_id in uids:\n",
    "    print(b_id)\n",
    "    input_vec = [(r[1]) for r in all_user_vecs if r[0] == b_id][0]\n",
    "    #input_vec = reviews_by_business_trf_df.select('word_vec')\\\n",
    "                #.filter(reviews_by_business_trf_df['business_id'] == b_id)\\\n",
    "                #.collect()[0][0]\n",
    "\n",
    "    similar_user_rdd = sc.parallelize((i[0], float(CosineSim(input_vec, i[1]))) for i in all_user_vecs)\n",
    "\n",
    "    similar_user_df = spark.createDataFrame(similar_user_rdd) \\\n",
    "        .withColumnRenamed('_1', 'user_id') \\\n",
    "        .withColumnRenamed('_2', 'score') \\\n",
    "        .orderBy(\"score\", ascending = False)\n",
    "    similar_user_df = similar_user_df.dropna()\n",
    "    #.where(isnan(col(\"a\")))\n",
    "    similar_user_df = similar_user_df.filter(col(\"user_id\") != b_id).limit(sim_user_limit)\n",
    "    similar_user_df = similar_user_df.withColumn('input_user_id', lit(b_id))\n",
    "\n",
    "    similar_user_df = similar_user_df \\\n",
    "                                .union(similar_user_df)\n",
    "\n",
    "    temp = temp.union(similar_user_df)\n",
    "    \n",
    "    print(\"-------------------------------------\")\n",
    "    print(temp.take(10))\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(user_id,StringType,true),StructField(score,DoubleType,true),StructField(input_user_id,StringType,false)))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_user_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(user_id='uO1w3qNo21c1bVHHFTYW0w', score=0.9722554149044773, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='myrcQ3h2G04Gv-ANG_oqrg', score=0.971381229778663, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='VVm-TFCpi9M1-k8ED0l1eA', score=0.9710125785824026, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='v1nRL_f2EQs7fjh_m0TdQQ', score=0.969187261441371, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='vJGLEHyhCs9V-5fAe-xx3w', score=0.9686216169669871, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='PGx4HvY5joEeqXzam6tO7A', score=0.9658080239271861, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='USUw_VT31ygQ9JkEc6nPsg', score=0.9653626156916343, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='yL60onlTh1BmJKVLli_K3g', score=0.9646044155730369, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='n4usc2DF4pd9fdi2wLDloQ', score=0.9645918982625399, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='ZBllYKrFzaI0I7v6Wl26Wg', score=0.9639987177918352, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='uO1w3qNo21c1bVHHFTYW0w', score=0.9722554149044773, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='myrcQ3h2G04Gv-ANG_oqrg', score=0.971381229778663, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='VVm-TFCpi9M1-k8ED0l1eA', score=0.9710125785824026, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='v1nRL_f2EQs7fjh_m0TdQQ', score=0.969187261441371, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='vJGLEHyhCs9V-5fAe-xx3w', score=0.9686216169669871, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='PGx4HvY5joEeqXzam6tO7A', score=0.9658080239271861, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='USUw_VT31ygQ9JkEc6nPsg', score=0.9653626156916343, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='yL60onlTh1BmJKVLli_K3g', score=0.9646044155730369, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='n4usc2DF4pd9fdi2wLDloQ', score=0.9645918982625399, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='ZBllYKrFzaI0I7v6Wl26Wg', score=0.9639987177918352, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='xcBcIMVWEx1p7V8ACaeHpQ', score=0.9186675960618103, input_user_id='QBac9-Ii6jR-yLsQ5MVTHg'), Row(user_id='kw-YtOKPXrRB2a9wRZlmzQ', score=0.9154528761077289, input_user_id='QBac9-Ii6jR-yLsQ5MVTHg'), Row(user_id='MpN81tQOL86GaFse-_tTRQ', score=0.913263481576842, input_user_id='QBac9-Ii6jR-yLsQ5MVTHg'), Row(user_id='cNhHuEQMIpLH_qc9qGz67A', score=0.9107741561512877, input_user_id='QBac9-Ii6jR-yLsQ5MVTHg'), Row(user_id='g5W7s0n19gvT1Ujy_ITJog', score=0.9078462909984318, input_user_id='QBac9-Ii6jR-yLsQ5MVTHg'), Row(user_id='J5Eb7LhJaOa20k0ppcOCOg', score=0.9076639247312002, input_user_id='QBac9-Ii6jR-yLsQ5MVTHg'), Row(user_id='_IR48ok0ZkPMWJ2PlRCk0A', score=0.9070984934892682, input_user_id='QBac9-Ii6jR-yLsQ5MVTHg'), Row(user_id='bPUpO-bP6BmAGvSwPyDsng', score=0.9058483330903827, input_user_id='QBac9-Ii6jR-yLsQ5MVTHg'), Row(user_id='eV5usRjY2cDqNKVv8wXroA', score=0.9050774344012574, input_user_id='QBac9-Ii6jR-yLsQ5MVTHg'), Row(user_id='zkKv10FEw-iWJBHvK0mGfA', score=0.9037325430059482, input_user_id='QBac9-Ii6jR-yLsQ5MVTHg'), Row(user_id='xcBcIMVWEx1p7V8ACaeHpQ', score=0.9186675960618103, input_user_id='QBac9-Ii6jR-yLsQ5MVTHg'), Row(user_id='kw-YtOKPXrRB2a9wRZlmzQ', score=0.9154528761077289, input_user_id='QBac9-Ii6jR-yLsQ5MVTHg'), Row(user_id='MpN81tQOL86GaFse-_tTRQ', score=0.913263481576842, input_user_id='QBac9-Ii6jR-yLsQ5MVTHg'), Row(user_id='cNhHuEQMIpLH_qc9qGz67A', score=0.9107741561512877, input_user_id='QBac9-Ii6jR-yLsQ5MVTHg'), Row(user_id='g5W7s0n19gvT1Ujy_ITJog', score=0.9078462909984318, input_user_id='QBac9-Ii6jR-yLsQ5MVTHg'), Row(user_id='J5Eb7LhJaOa20k0ppcOCOg', score=0.9076639247312002, input_user_id='QBac9-Ii6jR-yLsQ5MVTHg'), Row(user_id='_IR48ok0ZkPMWJ2PlRCk0A', score=0.9070984934892682, input_user_id='QBac9-Ii6jR-yLsQ5MVTHg'), Row(user_id='bPUpO-bP6BmAGvSwPyDsng', score=0.9058483330903827, input_user_id='QBac9-Ii6jR-yLsQ5MVTHg'), Row(user_id='eV5usRjY2cDqNKVv8wXroA', score=0.9050774344012574, input_user_id='QBac9-Ii6jR-yLsQ5MVTHg'), Row(user_id='zkKv10FEw-iWJBHvK0mGfA', score=0.9037325430059482, input_user_id='QBac9-Ii6jR-yLsQ5MVTHg')]\n"
     ]
    }
   ],
   "source": [
    "print(temp.take(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(user_id='J5Eb7LhJaOa20k0ppcOCOg', score=0.9076639247312002, input_user_id='QBac9-Ii6jR-yLsQ5MVTHg'), Row(user_id='ZBllYKrFzaI0I7v6Wl26Wg', score=0.9639987177918352, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='cNhHuEQMIpLH_qc9qGz67A', score=0.9107741561512877, input_user_id='QBac9-Ii6jR-yLsQ5MVTHg'), Row(user_id='_IR48ok0ZkPMWJ2PlRCk0A', score=0.9070984934892682, input_user_id='QBac9-Ii6jR-yLsQ5MVTHg'), Row(user_id='MpN81tQOL86GaFse-_tTRQ', score=0.913263481576842, input_user_id='QBac9-Ii6jR-yLsQ5MVTHg'), Row(user_id='kw-YtOKPXrRB2a9wRZlmzQ', score=0.9154528761077289, input_user_id='QBac9-Ii6jR-yLsQ5MVTHg'), Row(user_id='uO1w3qNo21c1bVHHFTYW0w', score=0.9722554149044773, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='v1nRL_f2EQs7fjh_m0TdQQ', score=0.969187261441371, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='g5W7s0n19gvT1Ujy_ITJog', score=0.9078462909984318, input_user_id='QBac9-Ii6jR-yLsQ5MVTHg'), Row(user_id='bPUpO-bP6BmAGvSwPyDsng', score=0.9058483330903827, input_user_id='QBac9-Ii6jR-yLsQ5MVTHg'), Row(user_id='xcBcIMVWEx1p7V8ACaeHpQ', score=0.9186675960618103, input_user_id='QBac9-Ii6jR-yLsQ5MVTHg'), Row(user_id='zkKv10FEw-iWJBHvK0mGfA', score=0.9037325430059482, input_user_id='QBac9-Ii6jR-yLsQ5MVTHg'), Row(user_id='PGx4HvY5joEeqXzam6tO7A', score=0.9658080239271861, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='VVm-TFCpi9M1-k8ED0l1eA', score=0.9710125785824026, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='yL60onlTh1BmJKVLli_K3g', score=0.9646044155730369, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='vJGLEHyhCs9V-5fAe-xx3w', score=0.9686216169669871, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='n4usc2DF4pd9fdi2wLDloQ', score=0.9645918982625399, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='myrcQ3h2G04Gv-ANG_oqrg', score=0.971381229778663, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='USUw_VT31ygQ9JkEc6nPsg', score=0.9653626156916343, input_user_id='nOTl4aPC4tKHK35T3bNauQ'), Row(user_id='eV5usRjY2cDqNKVv8wXroA', score=0.9050774344012574, input_user_id='QBac9-Ii6jR-yLsQ5MVTHg')]\n"
     ]
    }
   ],
   "source": [
    "print(temp.dropDuplicates().take(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
